model_id: mistralai/Mistral-7B-Instruct-v0.2  # swap to Qwen/Qwen2-3B-Instruct if OOM
use_qlora: true
seq_len: 1024
batch_size: 1
eval_batch_size: 1
grad_accum: 8
epochs: 1
max_steps: 800
eval_strategy: "no"
save_strategy: "epoch"
output_dir: results/models/mistral_lora
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
data: { train: data/processed/train.jsonl, dev: data/processed/dev.jsonl }