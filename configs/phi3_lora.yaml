model_id: microsoft/phi-3-mini-4k-instruct
seed: 42
seq_len: 512
batch_size: 1
eval_batch_size: 1
grad_accum: 16
epochs: 2
lr: 2.0e-4
warmup_ratio: 0.03
logging_steps: 50
eval_steps: 200
save_steps: 200
save_total_limit: 2
output_dir: results/models/phi3_lora
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj"]
data:
  train: data/processed/train.jsonl
  dev:   data/processed/dev.jsonl
